{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from datasets import Dataset \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate \n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/data_preprocessed.csv')\n",
    "dataset = Dataset.from_pandas(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pre-trained model and tokenizer \n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('JamesH/Movie_review_sentiment_analysis_model')\n",
    "tokenizer = AutoTokenizer.from_pretrained('JamesH/Movie_review_sentiment_analysis_model')\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ab58b85fd44104a454f2d9c951dbdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the dataset before train/test splitting and selecting a small subset for model fine tuning \n",
    "\n",
    "tokenized_ds = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_ds = tokenized_ds.train_test_split(test_size=0.2)\n",
    "\n",
    "small_train = tokenized_ds['train'].shuffle(seed=13).select(range(200))\n",
    "small_test = tokenized_ds['test'].shuffle(seed=13).select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "main_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "models_dir = os.path.abspath(os.path.join(main_dir, 'models')) # Creating a new directory for saving models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred \n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Evaluation metrics for classification\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=None)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    auc = roc_auc_score(labels, logits[:, 1])\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"auc\": auc\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=models_dir,\n",
    "    save_strategy='no',\n",
    "    eval_strategy='epoch',\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,                                                         \n",
    "    per_device_eval_batch_size=2, \n",
    "    fp16=True,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_8bit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                        # Your model (e.g., BertForSequenceClassification)\n",
    "    args=training_args,                # TrainingArguments\n",
    "    train_dataset=small_train,      # Your training dataset\n",
    "    eval_dataset=small_test,        # Optional: for evaluation\n",
    "    data_collator=data_collator,              # Optional: if using Hugging Face tokenizer\n",
    "    compute_metrics=compute_metrics,  # Optional: your custom metric function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 03:07, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.219215</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>[0.94897959 0.95098039]</td>\n",
       "      <td>[0.93939394 0.96039604]</td>\n",
       "      <td>[0.95876289 0.94174757]</td>\n",
       "      <td>0.987088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.491380</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>[0.9178744 0.9119171]</td>\n",
       "      <td>[0.86363636 0.97777778]</td>\n",
       "      <td>[0.97938144 0.85436893]</td>\n",
       "      <td>0.970473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.410881</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>[0.94 0.94]</td>\n",
       "      <td>[0.91262136 0.96907216]</td>\n",
       "      <td>[0.96907216 0.91262136]</td>\n",
       "      <td>0.978381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.432977</td>\n",
       "      <td>0.935000</td>\n",
       "      <td>[0.93532338 0.93467337]</td>\n",
       "      <td>[0.90384615 0.96875   ]</td>\n",
       "      <td>[0.96907216 0.90291262]</td>\n",
       "      <td>0.977480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=0.2230299186706543, metrics={'train_runtime': 188.5695, 'train_samples_per_second': 4.242, 'train_steps_per_second': 2.121, 'total_flos': 165157565750472.0, 'train_loss': 0.2230299186706543, 'epoch': 4.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.43297669291496277,\n",
       " 'eval_accuracy': 0.935,\n",
       " 'eval_f1': array([0.93532338, 0.93467337]),\n",
       " 'eval_precision': array([0.90384615, 0.96875   ]),\n",
       " 'eval_recall': array([0.96907216, 0.90291262]),\n",
       " 'eval_auc': 0.9774797317585827,\n",
       " 'eval_runtime': 10.5516,\n",
       " 'eval_samples_per_second': 18.954,\n",
       " 'eval_steps_per_second': 9.477,\n",
       " 'epoch': 4.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def op_compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred \n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Evaluation metrics for classification\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=None)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision_class_0\": float(precision[0]),\n",
    "        \"precision_class_1\": float(precision[1]),\n",
    "        \"recall_class_0\": float(recall[0]),\n",
    "        \"recall_class_1\": float(recall[1]),\n",
    "        \"f1_class_0\": float(f1[0]),\n",
    "        \"f1_class_1\": float(f1[1])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_training_args = TrainingArguments(\n",
    "    output_dir=models_dir,\n",
    "    save_strategy='no',\n",
    "    eval_strategy='epoch',\n",
    "    num_train_epochs=4, # no change from epoch 3 to epoch 4 during initial training\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,                                                         \n",
    "    per_device_eval_batch_size=2, \n",
    "    fp16=True,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_8bit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-06 00:07:27,088] A new study created in memory with name: no-name-a46bca1a-c203-4330-87f5-519b2f37fd71\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 04:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Class 0</th>\n",
       "      <th>Precision Class 1</th>\n",
       "      <th>Recall Class 0</th>\n",
       "      <th>Recall Class 1</th>\n",
       "      <th>F1 Class 0</th>\n",
       "      <th>F1 Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.740802</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.530055</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.165049</td>\n",
       "      <td>0.692857</td>\n",
       "      <td>0.283333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.251927</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.886792</td>\n",
       "      <td>0.968085</td>\n",
       "      <td>0.969072</td>\n",
       "      <td>0.883495</td>\n",
       "      <td>0.926108</td>\n",
       "      <td>0.923858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.362987</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.878505</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.969072</td>\n",
       "      <td>0.873786</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.918367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-06 00:12:04,034] Trial 0 finished with value: 6.449041155486267 and parameters: {'learning_rate': 7.200613184257478e-05, 'warmup_ratio': 0.06343566675585444, 'num_train_epochs': 3, 'gradient_accumulation_steps': 8}. Best is trial 0 with value: 6.449041155486267.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 10:00, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Class 0</th>\n",
       "      <th>Precision Class 1</th>\n",
       "      <th>Recall Class 0</th>\n",
       "      <th>Recall Class 1</th>\n",
       "      <th>F1 Class 0</th>\n",
       "      <th>F1 Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.768023</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.985714</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>0.669903</td>\n",
       "      <td>0.845815</td>\n",
       "      <td>0.797688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.303006</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.920792</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.958763</td>\n",
       "      <td>0.922330</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.940594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.319054</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.920792</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.958763</td>\n",
       "      <td>0.922330</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.940594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.329368</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.920792</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.958763</td>\n",
       "      <td>0.922330</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.940594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-06 00:22:06,766] Trial 1 finished with value: 6.581469021289077 and parameters: {'learning_rate': 1.4373369541741355e-05, 'warmup_ratio': 0.002241635644796569, 'num_train_epochs': 4, 'gradient_accumulation_steps': 2}. Best is trial 1 with value: 6.581469021289077.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 19:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Class 0</th>\n",
       "      <th>Precision Class 1</th>\n",
       "      <th>Recall Class 0</th>\n",
       "      <th>Recall Class 1</th>\n",
       "      <th>F1 Class 0</th>\n",
       "      <th>F1 Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.042246</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.700730</td>\n",
       "      <td>0.984127</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>0.601942</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.746988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.321819</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.930693</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.969072</td>\n",
       "      <td>0.932039</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.950495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.362938</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.969388</td>\n",
       "      <td>0.969072</td>\n",
       "      <td>0.922330</td>\n",
       "      <td>0.944724</td>\n",
       "      <td>0.945274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-06 00:41:43,494] Trial 2 finished with value: 6.617355894520101 and parameters: {'learning_rate': 1.8298827638569346e-05, 'warmup_ratio': 0.06384813740224214, 'num_train_epochs': 3, 'gradient_accumulation_steps': 1}. Best is trial 2 with value: 6.617355894520101.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 04:43, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Class 0</th>\n",
       "      <th>Precision Class 1</th>\n",
       "      <th>Recall Class 0</th>\n",
       "      <th>Recall Class 1</th>\n",
       "      <th>F1 Class 0</th>\n",
       "      <th>F1 Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.426808</td>\n",
       "      <td>0.885000</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.838983</td>\n",
       "      <td>0.804124</td>\n",
       "      <td>0.961165</td>\n",
       "      <td>0.871508</td>\n",
       "      <td>0.895928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.262356</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.936842</td>\n",
       "      <td>0.923810</td>\n",
       "      <td>0.917526</td>\n",
       "      <td>0.941748</td>\n",
       "      <td>0.927083</td>\n",
       "      <td>0.932692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-06 00:46:30,784] Trial 3 finished with value: 6.5097006161097335 and parameters: {'learning_rate': 7.084851738585837e-05, 'warmup_ratio': 0.06880538794097325, 'num_train_epochs': 2, 'gradient_accumulation_steps': 8}. Best is trial 2 with value: 6.617355894520101.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 46:48, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Class 0</th>\n",
       "      <th>Precision Class 1</th>\n",
       "      <th>Recall Class 0</th>\n",
       "      <th>Recall Class 1</th>\n",
       "      <th>F1 Class 0</th>\n",
       "      <th>F1 Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.211710</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.930693</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.969072</td>\n",
       "      <td>0.932039</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.950495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.255989</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.969388</td>\n",
       "      <td>0.969072</td>\n",
       "      <td>0.922330</td>\n",
       "      <td>0.944724</td>\n",
       "      <td>0.945274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.270911</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.958763</td>\n",
       "      <td>0.932039</td>\n",
       "      <td>0.944162</td>\n",
       "      <td>0.945813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.244580</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.948454</td>\n",
       "      <td>0.951456</td>\n",
       "      <td>0.948454</td>\n",
       "      <td>0.951456</td>\n",
       "      <td>0.948454</td>\n",
       "      <td>0.951456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-06 01:33:22,645] Trial 4 finished with value: 6.6497297567811025 and parameters: {'learning_rate': 1.2090707659718472e-05, 'warmup_ratio': 0.09356655990374085, 'num_train_epochs': 4, 'gradient_accumulation_steps': 4}. Best is trial 4 with value: 6.6497297567811025.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: BestRun(run_id='4', objective=6.6497297567811025, hyperparameters={'learning_rate': 1.2090707659718472e-05, 'warmup_ratio': 0.09356655990374085, 'num_train_epochs': 4, 'gradient_accumulation_steps': 4}, run_summary=None)\n"
     ]
    }
   ],
   "source": [
    "# Optimizing hyperparameters \n",
    "\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained('JamesH/Movie_review_sentiment_analysis_model')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=op_training_args,\n",
    "    train_dataset=small_train,\n",
    "    eval_dataset=small_test,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=op_compute_metrics,\n",
    ")\n",
    "# Define hyperparameter search space\n",
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-4, log=True),\n",
    "        \"warmup_ratio\": trial.suggest_float(\"warmup_ratio\", 0.0, 0.1),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 5),\n",
    "        \"gradient_accumulation_steps\": trial.suggest_categorical(\"gradient_accumulation_steps\", [1, 2, 4, 8])\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",  # Maximize metric \n",
    "    hp_space=hp_space,\n",
    "    backend=\"optuna\",\n",
    "    n_trials=5# Number of trials to run\n",
    ")\n",
    "\n",
    "print(f\"Best trial: {best_trial}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samhk\\anaconda3\\Lib\\site-packages\\transformers\\configuration_utils.py:407: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 512}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\samhk\\\\LLM-Project\\\\models\\\\finetuned_model\\\\tokenizer_config.json',\n",
       " 'C:\\\\Users\\\\samhk\\\\LLM-Project\\\\models\\\\finetuned_model\\\\special_tokens_map.json',\n",
       " 'C:\\\\Users\\\\samhk\\\\LLM-Project\\\\models\\\\finetuned_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a subdirectory for the tuned model and saving it \n",
    "\n",
    "ft_model = os.path.abspath(os.path.join(models_dir, 'finetuned_model'))\n",
    "  \n",
    "os.makedirs(ft_model, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(ft_model)\n",
    "tokenizer.save_pretrained(ft_model)         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
